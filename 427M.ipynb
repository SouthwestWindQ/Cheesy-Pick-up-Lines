{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa98654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import jieba\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import XLNetTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2d4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    lr = 3e-5\n",
    "    epoch = 10\n",
    "    batch_size = 16\n",
    "    max_length = 80\n",
    "    weight_decay = 0.01\n",
    "    output_dir = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbaf0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    import pandas as pd\n",
    "    from datasets import load_dataset\n",
    "    data = pd.read_excel(\"trial.xls\")\n",
    "    pd.DataFrame.to_csv(data, \"trial.csv\")\n",
    "    dataset = load_dataset('csv', data_files=\"trial.csv\")\n",
    "    dataset = dataset['train'].train_test_split(train_size=0.8)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f239aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83737e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetTokenizer(XLNetTokenizer):\n",
    "    translator = str.maketrans(\" \\n\", \"\\u2582\\u2583\")\n",
    "    def _tokenize(self, text, *args, **kwargs):\n",
    "        text = [x.translate(self.translator) for x in jieba.cut(text, cut_all=False)]\n",
    "        text = \" \".join(text)\n",
    "        return super()._tokenize(text, *args, **kwargs)\n",
    "    def _decode(self, *args, **kwargs):\n",
    "        text = super()._decode(*args, **kwargs)\n",
    "        text = text.replace(' ', '').replace('\\u2582', ' ').replace('\\u2583', '\\n')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6148738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        labels = inputs['input_ids']\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # 忽略pad_id的loss,并对所有的非pad_id的loss进行求和\n",
    "        loss_fct = CrossEntropyLoss(ignore_index=pad_id, reduction='sum')  \n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1ff1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CpmTokenizer'. \n",
      "The class this function is called from is 'XLNetTokenizer'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mymusise/CPM-Generate-distill'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "global pad_id\n",
    "pad_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ead51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bbb762a6384ebf4e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-bbb762a6384ebf4e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2910.69it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 193.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-bbb762a6384ebf4e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 471.64it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?ba/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.508 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.97ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.69ba/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = get_dataset()\n",
    "cls, sep = tokenizer.cls_token, tokenizer.sep_token\n",
    "datasets['train'] = datasets['train'].map( \n",
    "    lambda e: tokenizer(\n",
    "        [cls + e['keyword'][i] + sep + e['sentence'][i] + sep for i in range(len(e['keyword']))],\n",
    "        truncation=True, padding='max_length', max_length=args.max_length\n",
    "    ), batched=True,\n",
    ")\n",
    "datasets['train'].set_format(type='torch', columns=['input_ids'])\n",
    "datasets['test'] = datasets['test'].map( \n",
    "    lambda e: tokenizer(\n",
    "        [cls + e['keyword'][i] + sep + e['sentence'][i] + sep for i in range(len(e['keyword']))],\n",
    "        truncation=True, padding='max_length', max_length=args.max_length\n",
    "    ), batched=True,\n",
    ")\n",
    "datasets['test'].set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e904da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbf5217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1649\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1040/1040 01:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>859.949200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>859.949200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>859.949200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>859.949200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>859.949200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>374.552000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Unnamed: 0, sentence, keyword, Unnamed: 0.1. If Unnamed: 0, sentence, keyword, Unnamed: 0.1 are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 413\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1040, training_loss=605.3508610652043, metrics={'train_runtime': 98.9887, 'train_samples_per_second': 166.585, 'train_steps_per_second': 10.506, 'total_flos': 673235251200000.0, 'train_loss': 605.3508610652043, 'epoch': 10.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=args.lr,\n",
    "    output_dir=args.output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=args.epoch,\n",
    "    weight_decay=args.weight_decay,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    eval_dataset=datasets['test'],\n",
    "    train_dataset=datasets['train'],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0d24ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词: 晚安\n",
      "土味情话: 你知道我的爱有多深吗?我的爱就像大海,波澜不惊,只是为了见你一眼,就已经蓄谋已久。"
     ]
    }
   ],
   "source": [
    "from transformers import TextGenerationPipeline\n",
    "model = model.cpu()\n",
    "text_generater = TextGenerationPipeline(model, tokenizer)\n",
    "keyword = \"晚安\"\n",
    "print('关键词:', keyword)\n",
    "keyword = tokenizer.cls_token + keyword + tokenizer.sep_token\n",
    "text = text_generater(keyword, max_length=80, top_k=1, use_cache=True, prefix='')[0]['generated_text']\n",
    "text = text[len(keyword):]\n",
    "print('土味情话:', end=' ')\n",
    "for s in text:\n",
    "    print(s, end='')\n",
    "    if s == tokenizer.sep_token or s == '。':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d705c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
